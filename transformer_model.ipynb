{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coFcmMNEYs6s"
      },
      "source": [
        "# CSC412 Project (transformer part)\n",
        "#### Author: Xinyi Ji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgxnB9zY2i8"
      },
      "source": [
        "I write the code mainly based on these youtube videos:\n",
        "- https://www.youtube.com/watch?v=U0s0f995w14\n",
        "- http://jalammar.github.io/illustrated-transformer/\n",
        "- https://www.youtube.com/watch?v=4Bdc55j80l8\n",
        "- https://www.youtube.com/watch?app=desktop&v=dichIcUZfOw&ab_channel=Hedu-MathofIntelligence\n",
        "\n",
        "And of course the transformer paper \"Attention is all you need\".\n",
        "\n",
        "The whole model and traning process is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4K_6g3JZ4D5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import pandas as pd\n",
        "import torch.utils.data as data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37iq9GWf0YLS"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "w_l86AwIYOnN",
        "outputId": "5ba8f6a5-bf48-4f91-a34a-b0b2614e32c1"
      },
      "source": [
        "df = pd.read_csv('transformer_data.csv')\n",
        "df.columns = range(256+1024)\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1240</th>\n",
              "      <th>1241</th>\n",
              "      <th>1242</th>\n",
              "      <th>1243</th>\n",
              "      <th>1244</th>\n",
              "      <th>1245</th>\n",
              "      <th>1246</th>\n",
              "      <th>1247</th>\n",
              "      <th>1248</th>\n",
              "      <th>1249</th>\n",
              "      <th>1250</th>\n",
              "      <th>1251</th>\n",
              "      <th>1252</th>\n",
              "      <th>1253</th>\n",
              "      <th>1254</th>\n",
              "      <th>1255</th>\n",
              "      <th>1256</th>\n",
              "      <th>1257</th>\n",
              "      <th>1258</th>\n",
              "      <th>1259</th>\n",
              "      <th>1260</th>\n",
              "      <th>1261</th>\n",
              "      <th>1262</th>\n",
              "      <th>1263</th>\n",
              "      <th>1264</th>\n",
              "      <th>1265</th>\n",
              "      <th>1266</th>\n",
              "      <th>1267</th>\n",
              "      <th>1268</th>\n",
              "      <th>1269</th>\n",
              "      <th>1270</th>\n",
              "      <th>1271</th>\n",
              "      <th>1272</th>\n",
              "      <th>1273</th>\n",
              "      <th>1274</th>\n",
              "      <th>1275</th>\n",
              "      <th>1276</th>\n",
              "      <th>1277</th>\n",
              "      <th>1278</th>\n",
              "      <th>1279</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5232</td>\n",
              "      <td>209</td>\n",
              "      <td>2676</td>\n",
              "      <td>109</td>\n",
              "      <td>8031</td>\n",
              "      <td>127</td>\n",
              "      <td>4605</td>\n",
              "      <td>6384</td>\n",
              "      <td>510</td>\n",
              "      <td>3022</td>\n",
              "      <td>6271</td>\n",
              "      <td>2497</td>\n",
              "      <td>3224</td>\n",
              "      <td>4407</td>\n",
              "      <td>2006</td>\n",
              "      <td>5214</td>\n",
              "      <td>2629</td>\n",
              "      <td>5629</td>\n",
              "      <td>5214</td>\n",
              "      <td>5536</td>\n",
              "      <td>5643</td>\n",
              "      <td>4972</td>\n",
              "      <td>1177</td>\n",
              "      <td>1087</td>\n",
              "      <td>3414</td>\n",
              "      <td>5536</td>\n",
              "      <td>4547</td>\n",
              "      <td>7628</td>\n",
              "      <td>5239</td>\n",
              "      <td>1255</td>\n",
              "      <td>6583</td>\n",
              "      <td>7148</td>\n",
              "      <td>6565</td>\n",
              "      <td>3188</td>\n",
              "      <td>4312</td>\n",
              "      <td>6262</td>\n",
              "      <td>448</td>\n",
              "      <td>1732</td>\n",
              "      <td>215</td>\n",
              "      <td>4120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2557</td>\n",
              "      <td>1971</td>\n",
              "      <td>5385</td>\n",
              "      <td>30</td>\n",
              "      <td>2336</td>\n",
              "      <td>1778</td>\n",
              "      <td>199</td>\n",
              "      <td>560</td>\n",
              "      <td>6674</td>\n",
              "      <td>2532</td>\n",
              "      <td>7596</td>\n",
              "      <td>3123</td>\n",
              "      <td>2558</td>\n",
              "      <td>7453</td>\n",
              "      <td>5045</td>\n",
              "      <td>3927</td>\n",
              "      <td>2483</td>\n",
              "      <td>5698</td>\n",
              "      <td>2862</td>\n",
              "      <td>6165</td>\n",
              "      <td>5517</td>\n",
              "      <td>170</td>\n",
              "      <td>6460</td>\n",
              "      <td>5813</td>\n",
              "      <td>5897</td>\n",
              "      <td>3230</td>\n",
              "      <td>3533</td>\n",
              "      <td>7976</td>\n",
              "      <td>684</td>\n",
              "      <td>6770</td>\n",
              "      <td>1291</td>\n",
              "      <td>5470</td>\n",
              "      <td>3610</td>\n",
              "      <td>2133</td>\n",
              "      <td>6686</td>\n",
              "      <td>108</td>\n",
              "      <td>4834</td>\n",
              "      <td>3610</td>\n",
              "      <td>5542</td>\n",
              "      <td>1244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6791</td>\n",
              "      <td>4609</td>\n",
              "      <td>1917</td>\n",
              "      <td>3862</td>\n",
              "      <td>943</td>\n",
              "      <td>5916</td>\n",
              "      <td>1601</td>\n",
              "      <td>540</td>\n",
              "      <td>5363</td>\n",
              "      <td>4448</td>\n",
              "      <td>390</td>\n",
              "      <td>2560</td>\n",
              "      <td>2786</td>\n",
              "      <td>1147</td>\n",
              "      <td>4552</td>\n",
              "      <td>3916</td>\n",
              "      <td>671</td>\n",
              "      <td>2410</td>\n",
              "      <td>6791</td>\n",
              "      <td>6648</td>\n",
              "      <td>6875</td>\n",
              "      <td>7870</td>\n",
              "      <td>3830</td>\n",
              "      <td>3152</td>\n",
              "      <td>2114</td>\n",
              "      <td>5479</td>\n",
              "      <td>6443</td>\n",
              "      <td>6304</td>\n",
              "      <td>4283</td>\n",
              "      <td>6850</td>\n",
              "      <td>3441</td>\n",
              "      <td>6419</td>\n",
              "      <td>2014</td>\n",
              "      <td>390</td>\n",
              "      <td>4809</td>\n",
              "      <td>6227</td>\n",
              "      <td>5031</td>\n",
              "      <td>1680</td>\n",
              "      <td>1018</td>\n",
              "      <td>1309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6004</td>\n",
              "      <td>5403</td>\n",
              "      <td>7180</td>\n",
              "      <td>5668</td>\n",
              "      <td>2653</td>\n",
              "      <td>2327</td>\n",
              "      <td>5689</td>\n",
              "      <td>7781</td>\n",
              "      <td>6958</td>\n",
              "      <td>1600</td>\n",
              "      <td>7823</td>\n",
              "      <td>4007</td>\n",
              "      <td>6721</td>\n",
              "      <td>4621</td>\n",
              "      <td>4950</td>\n",
              "      <td>2690</td>\n",
              "      <td>5175</td>\n",
              "      <td>2684</td>\n",
              "      <td>2418</td>\n",
              "      <td>2952</td>\n",
              "      <td>3096</td>\n",
              "      <td>7012</td>\n",
              "      <td>4800</td>\n",
              "      <td>5477</td>\n",
              "      <td>2737</td>\n",
              "      <td>3851</td>\n",
              "      <td>7463</td>\n",
              "      <td>101</td>\n",
              "      <td>4583</td>\n",
              "      <td>7189</td>\n",
              "      <td>3404</td>\n",
              "      <td>5868</td>\n",
              "      <td>383</td>\n",
              "      <td>2548</td>\n",
              "      <td>6361</td>\n",
              "      <td>2576</td>\n",
              "      <td>2119</td>\n",
              "      <td>8011</td>\n",
              "      <td>4340</td>\n",
              "      <td>6000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>4979</td>\n",
              "      <td>6587</td>\n",
              "      <td>6241</td>\n",
              "      <td>6473</td>\n",
              "      <td>4759</td>\n",
              "      <td>1830</td>\n",
              "      <td>5048</td>\n",
              "      <td>332</td>\n",
              "      <td>4472</td>\n",
              "      <td>2791</td>\n",
              "      <td>795</td>\n",
              "      <td>6322</td>\n",
              "      <td>6583</td>\n",
              "      <td>8148</td>\n",
              "      <td>5718</td>\n",
              "      <td>3759</td>\n",
              "      <td>5998</td>\n",
              "      <td>1534</td>\n",
              "      <td>5951</td>\n",
              "      <td>2849</td>\n",
              "      <td>4234</td>\n",
              "      <td>6101</td>\n",
              "      <td>6330</td>\n",
              "      <td>541</td>\n",
              "      <td>5978</td>\n",
              "      <td>2433</td>\n",
              "      <td>4950</td>\n",
              "      <td>5608</td>\n",
              "      <td>4165</td>\n",
              "      <td>5623</td>\n",
              "      <td>2177</td>\n",
              "      <td>2576</td>\n",
              "      <td>2464</td>\n",
              "      <td>825</td>\n",
              "      <td>4920</td>\n",
              "      <td>6289</td>\n",
              "      <td>61</td>\n",
              "      <td>4492</td>\n",
              "      <td>2061</td>\n",
              "      <td>704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5516</td>\n",
              "      <td>1929</td>\n",
              "      <td>7510</td>\n",
              "      <td>4785</td>\n",
              "      <td>6265</td>\n",
              "      <td>5156</td>\n",
              "      <td>5725</td>\n",
              "      <td>4155</td>\n",
              "      <td>2343</td>\n",
              "      <td>2063</td>\n",
              "      <td>6559</td>\n",
              "      <td>4736</td>\n",
              "      <td>3054</td>\n",
              "      <td>2190</td>\n",
              "      <td>2047</td>\n",
              "      <td>7133</td>\n",
              "      <td>5951</td>\n",
              "      <td>6710</td>\n",
              "      <td>8019</td>\n",
              "      <td>4734</td>\n",
              "      <td>5807</td>\n",
              "      <td>7022</td>\n",
              "      <td>5288</td>\n",
              "      <td>202</td>\n",
              "      <td>5650</td>\n",
              "      <td>1698</td>\n",
              "      <td>3959</td>\n",
              "      <td>2826</td>\n",
              "      <td>4377</td>\n",
              "      <td>10</td>\n",
              "      <td>2167</td>\n",
              "      <td>7537</td>\n",
              "      <td>6297</td>\n",
              "      <td>4441</td>\n",
              "      <td>7141</td>\n",
              "      <td>729</td>\n",
              "      <td>5452</td>\n",
              "      <td>4844</td>\n",
              "      <td>3572</td>\n",
              "      <td>6243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2805</td>\n",
              "      <td>5466</td>\n",
              "      <td>5835</td>\n",
              "      <td>2161</td>\n",
              "      <td>2793</td>\n",
              "      <td>5573</td>\n",
              "      <td>5172</td>\n",
              "      <td>3754</td>\n",
              "      <td>7882</td>\n",
              "      <td>1672</td>\n",
              "      <td>2657</td>\n",
              "      <td>5324</td>\n",
              "      <td>4670</td>\n",
              "      <td>4442</td>\n",
              "      <td>7389</td>\n",
              "      <td>2924</td>\n",
              "      <td>8125</td>\n",
              "      <td>7468</td>\n",
              "      <td>4139</td>\n",
              "      <td>6310</td>\n",
              "      <td>7014</td>\n",
              "      <td>5782</td>\n",
              "      <td>1615</td>\n",
              "      <td>2448</td>\n",
              "      <td>1254</td>\n",
              "      <td>35</td>\n",
              "      <td>3454</td>\n",
              "      <td>225</td>\n",
              "      <td>1677</td>\n",
              "      <td>4575</td>\n",
              "      <td>5377</td>\n",
              "      <td>5668</td>\n",
              "      <td>4580</td>\n",
              "      <td>5891</td>\n",
              "      <td>6103</td>\n",
              "      <td>7455</td>\n",
              "      <td>7689</td>\n",
              "      <td>421</td>\n",
              "      <td>7673</td>\n",
              "      <td>2918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4509</td>\n",
              "      <td>202</td>\n",
              "      <td>3071</td>\n",
              "      <td>6258</td>\n",
              "      <td>2831</td>\n",
              "      <td>4522</td>\n",
              "      <td>7673</td>\n",
              "      <td>3046</td>\n",
              "      <td>4106</td>\n",
              "      <td>592</td>\n",
              "      <td>6258</td>\n",
              "      <td>5623</td>\n",
              "      <td>4670</td>\n",
              "      <td>4419</td>\n",
              "      <td>7321</td>\n",
              "      <td>7451</td>\n",
              "      <td>1787</td>\n",
              "      <td>2896</td>\n",
              "      <td>960</td>\n",
              "      <td>4670</td>\n",
              "      <td>592</td>\n",
              "      <td>592</td>\n",
              "      <td>592</td>\n",
              "      <td>592</td>\n",
              "      <td>3260</td>\n",
              "      <td>592</td>\n",
              "      <td>592</td>\n",
              "      <td>2896</td>\n",
              "      <td>916</td>\n",
              "      <td>6295</td>\n",
              "      <td>4106</td>\n",
              "      <td>4522</td>\n",
              "      <td>2896</td>\n",
              "      <td>592</td>\n",
              "      <td>592</td>\n",
              "      <td>3260</td>\n",
              "      <td>7986</td>\n",
              "      <td>2896</td>\n",
              "      <td>2830</td>\n",
              "      <td>1938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1304</td>\n",
              "      <td>6861</td>\n",
              "      <td>4305</td>\n",
              "      <td>4888</td>\n",
              "      <td>6958</td>\n",
              "      <td>7942</td>\n",
              "      <td>1950</td>\n",
              "      <td>1788</td>\n",
              "      <td>3641</td>\n",
              "      <td>7640</td>\n",
              "      <td>2756</td>\n",
              "      <td>3672</td>\n",
              "      <td>2736</td>\n",
              "      <td>5295</td>\n",
              "      <td>6363</td>\n",
              "      <td>3685</td>\n",
              "      <td>7697</td>\n",
              "      <td>1841</td>\n",
              "      <td>1415</td>\n",
              "      <td>615</td>\n",
              "      <td>7314</td>\n",
              "      <td>2080</td>\n",
              "      <td>7583</td>\n",
              "      <td>4748</td>\n",
              "      <td>6141</td>\n",
              "      <td>865</td>\n",
              "      <td>213</td>\n",
              "      <td>2707</td>\n",
              "      <td>4042</td>\n",
              "      <td>6928</td>\n",
              "      <td>7479</td>\n",
              "      <td>974</td>\n",
              "      <td>5778</td>\n",
              "      <td>7583</td>\n",
              "      <td>5375</td>\n",
              "      <td>15</td>\n",
              "      <td>5995</td>\n",
              "      <td>2399</td>\n",
              "      <td>7804</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6789</td>\n",
              "      <td>4843</td>\n",
              "      <td>3314</td>\n",
              "      <td>1928</td>\n",
              "      <td>6697</td>\n",
              "      <td>5844</td>\n",
              "      <td>168</td>\n",
              "      <td>3694</td>\n",
              "      <td>7745</td>\n",
              "      <td>4422</td>\n",
              "      <td>4515</td>\n",
              "      <td>5923</td>\n",
              "      <td>4545</td>\n",
              "      <td>48</td>\n",
              "      <td>1183</td>\n",
              "      <td>7437</td>\n",
              "      <td>3548</td>\n",
              "      <td>2090</td>\n",
              "      <td>1940</td>\n",
              "      <td>6141</td>\n",
              "      <td>1789</td>\n",
              "      <td>2957</td>\n",
              "      <td>2219</td>\n",
              "      <td>4574</td>\n",
              "      <td>1951</td>\n",
              "      <td>3320</td>\n",
              "      <td>6483</td>\n",
              "      <td>4883</td>\n",
              "      <td>2131</td>\n",
              "      <td>7910</td>\n",
              "      <td>6301</td>\n",
              "      <td>6638</td>\n",
              "      <td>782</td>\n",
              "      <td>2461</td>\n",
              "      <td>7362</td>\n",
              "      <td>5679</td>\n",
              "      <td>3680</td>\n",
              "      <td>2634</td>\n",
              "      <td>5798</td>\n",
              "      <td>6053</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows Ã— 1280 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2     3     4     5     ...  1274  1275  1276  1277  1278  1279\n",
              "0        0     0     0     0     0     0  ...  4312  6262   448  1732   215  4120\n",
              "1        0     0     0     0     0     0  ...  6686   108  4834  3610  5542  1244\n",
              "2        0     0     0     0     0     0  ...  4809  6227  5031  1680  1018  1309\n",
              "3        0     0     0     0     0     0  ...  6361  2576  2119  8011  4340  6000\n",
              "4        0     0     0     0     0     0  ...  4920  6289    61  4492  2061   704\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "3995     0     0     0     0     0     0  ...  7141   729  5452  4844  3572  6243\n",
              "3996     0     0     0     0     0     0  ...  6103  7455  7689   421  7673  2918\n",
              "3997     0     0     0     0     0     0  ...   592  3260  7986  2896  2830  1938\n",
              "3998     0     0     0     0     0     0  ...  5375    15  5995  2399  7804    20\n",
              "3999     0     0     0     0     0     0  ...  7362  5679  3680  2634  5798  6053\n",
              "\n",
              "[4000 rows x 1280 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoQL_lHkYe5H"
      },
      "source": [
        "txt_col = range(256)\n",
        "img_col = range(256,1024+256)\n",
        "df_txt = df[txt_col]\n",
        "df_img = df[img_col]\n",
        "txt_array = df_txt.values\n",
        "img_array = df_img.values\n",
        "all_data = df.values\n",
        "all_data = torch.from_numpy(all_data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y2YhDIorU8b",
        "outputId": "382be337-32d9-459f-9689-7acde3e57223"
      },
      "source": [
        "all_data.shape[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INMRt3O7VGB8"
      },
      "source": [
        "def split_train_test(data, train_fraq=0.9): \n",
        " n_samples = data.shape[0] \n",
        " data_train = data[:int(n_samples * train_fraq)] \n",
        " data_test = data[int(n_samples * train_fraq):] \n",
        " return data_train, data_test "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4idmVl0FrIPY"
      },
      "source": [
        "all_data = split_train_test(all_data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlqe3Jlsrp_2",
        "outputId": "6364dddb-2e7b-4913-b5c4-78cb4992d0cf"
      },
      "source": [
        "all_data[0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3600, 1280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPDy3-4Dr7yV"
      },
      "source": [
        "train_X = all_data[0][:, :256]\n",
        "train_y = all_data[0][:,256:]\n",
        "test_X =  all_data[1][:, :256]\n",
        "test_y = all_data[1][:,256:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTUEOKcu0eph"
      },
      "source": [
        "def dataloader(dataset, **kwargs): \n",
        "  if 'shuffle' not in kwargs: \n",
        "    kwargs['shuffle'] = True \n",
        "  if 'drop_last' not in kwargs: \n",
        "    kwargs['drop_last'] = True \n",
        "  if 'batch_size' not in kwargs: \n",
        "    kwargs['batch_size'] = 32 \n",
        "  if 'num_workers' not in kwargs: \n",
        "    kwargs['num_workers'] = 8 \n",
        "    kwargs['batch_size'] = min(kwargs['batch_size'], len(dataset)) \n",
        "  return data.DataLoader(dataset, **kwargs) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-vn7Bzc0Xed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16235286-ea08-45f3-f4c5-834fe44a7efa"
      },
      "source": [
        "# Train data is a tensor\n",
        "batch_size = 10\n",
        "num_workers = 8\n",
        "train_dataset = data.TensorDataset(train_X, train_y) \n",
        "test_dataset = data.TensorDataset(test_X, test_y) \n",
        "train_dataloader = dataloader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True) \n",
        "test_dataloader = dataloader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPHnzR0u0LBP"
      },
      "source": [
        "## Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYp5KqMEZ7Nw"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCr_LQk2aDyx"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9fPAnrVaIHp"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length = 256,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opgOtpa8aN2u"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj0zctuvaT1e"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length = 1024,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, row_mask,col_mask,conv_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        x = self.layers[0](x, enc_out, enc_out, src_mask, row_mask)\n",
        "        x = self.layers[1](x, enc_out, enc_out, src_mask, col_mask)\n",
        "        x = self.layers[2](x, enc_out, enc_out, src_mask, row_mask)\n",
        "        x = self.layers[3](x, enc_out, enc_out, src_mask, row_mask)\n",
        "        x = self.layers[4](x, enc_out, enc_out, src_mask, conv_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSn6pWBvaYXE"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        device=\"gpu\",\n",
        "        max_length=1024,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            256,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            1024,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_row_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "\n",
        "        # trg_len = 32 * 32 = 1024\n",
        "        assert (\n",
        "            1024 == trg_len\n",
        "        ), \"The size of image vector is 1024\"\n",
        "\n",
        "        y_axis = torch.range(0, 1023).expand(1024,1024)\n",
        "        x_axis = torch.transpose(torch.range(0, 1023).expand(1024,1024), 0, 1)\n",
        "        trg_mask = ((y_axis<x_axis).long() + (y_axis+32>x_axis).long() -1).bool()\n",
        "\n",
        "        trg_mask = trg_mask.expand(N, 1, trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "    \n",
        "    def make_col_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        # trg_len = 32 * 32 = 1024\n",
        "\n",
        "        assert (\n",
        "            1024 == trg_len\n",
        "        ), \"The size of image vector is 1024\"\n",
        "\n",
        "        trg_mask = torch.zeros(1024, 1024)\n",
        "        for i in range(32):\n",
        "          cur_dia = torch.ones((1, 1024-32*i))[0]\n",
        "          cur_mask = torch.diag(cur_dia,-32*i)\n",
        "          trg_mask = trg_mask + cur_mask\n",
        "\n",
        "        trg_mask = trg_mask.expand(N, 1, trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "    \n",
        "    def make_conv_mask(self, trg):\n",
        "      # I am not sure how to deal with this(I searched online without finding the result),hence I will use row mask instead as they have similar performance\n",
        "        N, trg_len = trg.shape\n",
        "        # trg_len = 32 * 32 = 1024\n",
        "        assert (\n",
        "            1024 == trg_len\n",
        "        ), \"The size of image vector is 1024\"\n",
        "\n",
        "        y_axis = torch.range(0, 1023).expand(1024,1024)\n",
        "        x_axis = torch.transpose(torch.range(0, 1023).expand(1024,1024), 0, 1)\n",
        "        trg_mask = ((y_axis<x_axis).long() + (y_axis+32>x_axis).long() -1).bool()\n",
        "\n",
        "        trg_mask = trg_mask.expand(N, 1, trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src_mask = self.make_src_mask(src)\n",
        "        # trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        row_mask = self.make_row_mask(trg)\n",
        "        col_mask = self.make_col_mask(trg)\n",
        "        conv_mask = self.make_conv_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, row_mask,col_mask,conv_mask)\n",
        "        return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xnapAow0RsI"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxUg1sHCXAy3"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "# x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
        "# trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "# src_pad_idx = 0\n",
        "# trg_pad_idx = 0\n",
        "# src_vocab_size = 10\n",
        "# trg_vocab_size = 10\n",
        "# model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "# out = model(x, trg[:, :-1])\n",
        "# print(out.shape)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTFQTLitSkUD"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# input = torch.tensor([[[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8]],[[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8]]],dtype=torch.float)\n",
        "# input = torch.transpose(input, 1, 2)\n",
        "# target = torch.tensor([[0,1,2],[2,3,1]], dtype=torch.long)\n",
        "# criterion(input, target).item()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21k7kcmzqm1j"
      },
      "source": [
        "# input  # batch=2 * voc=4(C) * ques_len=3"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abi_rYfmu7Bm"
      },
      "source": [
        "# target   # batch=2  * ques_len=3"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo7qQbYX-BYi"
      },
      "source": [
        "def loss_function(org_img, trg_img):\n",
        "  org_img = org_img[:,1:] # batch(N) * ques_len\n",
        "  trg_img = trg_img[:,:-1, :] # batch(N) * ques_len * voc_size\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  input = trg_img\n",
        "  input = torch.transpose(input, 1, 2)\n",
        "  target = org_img\n",
        "  return criterion(input, target)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hvwr-VSwxDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089ddb0c-61f2-4a98-f445-846a27690b9e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
        "print(device)\n",
        "\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 51\n",
        "trg_vocab_size = 8192\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Start training VAE...\")\n",
        "model.train()\n",
        "\n",
        "global_step = 0\n",
        "pause_iters = 1\n",
        "for epoch in range(100):\n",
        "    overall_loss = 0\n",
        "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
        "        x = x.to(device) \n",
        "        y = y.to(device) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        trg_img = model(x, y) # (N, query_len, trg_vocab_size) \n",
        "        loss = loss_function(y, trg_img)\n",
        "        \n",
        "        overall_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage traning Loss: \", overall_loss / (batch_idx*batch_size))\n",
        "    train_loss_list.append(overall_loss / (batch_idx*batch_size))\n",
        "\n",
        "    global_step += 1\n",
        "    if global_step % pause_iters == 0:\n",
        "      test_loss = 0\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch_idx, (x, y) in enumerate(test_dataloader):\n",
        "          x = x.to(device) \n",
        "          y = y.to(device) \n",
        "          trg_img = model(x, y) # (N, query_len, trg_vocab_size) \n",
        "          loss = loss_function(y, trg_img)\n",
        "          test_loss += loss.item()   \n",
        "        print(\"\\tAverage validation Loss: \", test_loss / (batch_idx*batch_size))\n",
        "        test_loss_list.append(test_loss / (batch_idx*batch_size))\n",
        "        model.train()\n",
        "print(\"Finish!!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Start training VAE...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch 1 complete! \tAverage traning Loss:  0.8783163073335185\n",
            "\tAverage validation Loss:  0.902344412681384\n",
            "\tEpoch 2 complete! \tAverage traning Loss:  0.8761059628223642\n",
            "\tAverage validation Loss:  0.9012905023036859\n",
            "\tEpoch 3 complete! \tAverage traning Loss:  0.8759795024202395\n",
            "\tAverage validation Loss:  0.9008156507443159\n",
            "\tEpoch 4 complete! \tAverage traning Loss:  0.8654701612785998\n",
            "\tAverage validation Loss:  0.881072814647968\n",
            "\tEpoch 5 complete! \tAverage traning Loss:  0.8434814989732833\n",
            "\tAverage validation Loss:  0.8678542650662936\n",
            "\tEpoch 6 complete! \tAverage traning Loss:  0.8279045253743036\n",
            "\tAverage validation Loss:  0.8536655108133951\n",
            "\tEpoch 7 complete! \tAverage traning Loss:  0.8164356074957463\n",
            "\tAverage validation Loss:  0.8461380469493377\n",
            "\tEpoch 8 complete! \tAverage traning Loss:  0.8078529441589101\n",
            "\tAverage validation Loss:  0.8401278202350323\n",
            "\tEpoch 9 complete! \tAverage traning Loss:  0.8004614431545927\n",
            "\tAverage validation Loss:  0.8379301144526555\n",
            "\tEpoch 10 complete! \tAverage traning Loss:  0.7936076480366061\n",
            "\tAverage validation Loss:  0.8321780522664388\n",
            "\tEpoch 11 complete! \tAverage traning Loss:  0.7866447616088357\n",
            "\tAverage validation Loss:  0.8285952788132888\n",
            "\tEpoch 12 complete! \tAverage traning Loss:  0.7801158027064501\n",
            "\tAverage validation Loss:  0.8281813756013527\n",
            "\tEpoch 13 complete! \tAverage traning Loss:  0.7739746646296679\n",
            "\tAverage validation Loss:  0.8261334896087646\n",
            "\tEpoch 14 complete! \tAverage traning Loss:  0.7681945338554701\n",
            "\tAverage validation Loss:  0.8265311326736059\n",
            "\tEpoch 15 complete! \tAverage traning Loss:  0.7628058622142398\n",
            "\tAverage validation Loss:  0.8262991012671055\n",
            "\tEpoch 16 complete! \tAverage traning Loss:  0.7573537142495923\n",
            "\tAverage validation Loss:  0.827178527147342\n",
            "\tEpoch 17 complete! \tAverage traning Loss:  0.7521922528577715\n",
            "\tAverage validation Loss:  0.8277879115862724\n",
            "\tEpoch 18 complete! \tAverage traning Loss:  0.7468539656370795\n",
            "\tAverage validation Loss:  0.8286752774165227\n",
            "\tEpoch 19 complete! \tAverage traning Loss:  0.7418525026369228\n",
            "\tAverage validation Loss:  0.8318806917239459\n",
            "\tEpoch 20 complete! \tAverage traning Loss:  0.7365833787200843\n",
            "\tAverage validation Loss:  0.8334637544093988\n",
            "\tEpoch 21 complete! \tAverage traning Loss:  0.7311635848871513\n",
            "\tAverage validation Loss:  0.8354071641579652\n",
            "\tEpoch 22 complete! \tAverage traning Loss:  0.7258545663031363\n",
            "\tAverage validation Loss:  0.8387648521325527\n",
            "\tEpoch 23 complete! \tAverage traning Loss:  0.7205555780352324\n",
            "\tAverage validation Loss:  0.8416186491648356\n",
            "\tEpoch 24 complete! \tAverage traning Loss:  0.7150436029460769\n",
            "\tAverage validation Loss:  0.844713116914798\n",
            "\tEpoch 25 complete! \tAverage traning Loss:  0.7096474825505759\n",
            "\tAverage validation Loss:  0.8488406059069511\n",
            "\tEpoch 26 complete! \tAverage traning Loss:  0.7039111480075336\n",
            "\tAverage validation Loss:  0.8541262968992576\n",
            "\tEpoch 27 complete! \tAverage traning Loss:  0.6983967928501225\n",
            "\tAverage validation Loss:  0.8567529861743634\n",
            "\tEpoch 28 complete! \tAverage traning Loss:  0.6926196511409409\n",
            "\tAverage validation Loss:  0.8612139200552916\n",
            "\tEpoch 29 complete! \tAverage traning Loss:  0.6870804231478975\n",
            "\tAverage validation Loss:  0.8670154449267266\n",
            "\tEpoch 30 complete! \tAverage traning Loss:  0.6809865375083137\n",
            "\tAverage validation Loss:  0.8728290142157139\n",
            "\tEpoch 31 complete! \tAverage traning Loss:  0.675297211139647\n",
            "\tAverage validation Loss:  0.8764281810858311\n",
            "\tEpoch 32 complete! \tAverage traning Loss:  0.6693507890515341\n",
            "\tAverage validation Loss:  0.883499639462202\n",
            "\tEpoch 33 complete! \tAverage traning Loss:  0.6638916042189744\n",
            "\tAverage validation Loss:  0.8880368208273863\n",
            "\tEpoch 34 complete! \tAverage traning Loss:  0.6578669704766659\n",
            "\tAverage validation Loss:  0.8961264781462841\n",
            "\tEpoch 35 complete! \tAverage traning Loss:  0.6521951367264006\n",
            "\tAverage validation Loss:  0.9009925793378781\n",
            "\tEpoch 36 complete! \tAverage traning Loss:  0.646185821543829\n",
            "\tAverage validation Loss:  0.9070602881602752\n",
            "\tEpoch 37 complete! \tAverage traning Loss:  0.6403220852769517\n",
            "\tAverage validation Loss:  0.9121206821539463\n",
            "\tEpoch 38 complete! \tAverage traning Loss:  0.6347678053013793\n",
            "\tAverage validation Loss:  0.919122977134509\n",
            "\tEpoch 39 complete! \tAverage traning Loss:  0.6290153895247945\n",
            "\tAverage validation Loss:  0.9255392930446527\n",
            "\tEpoch 40 complete! \tAverage traning Loss:  0.6230868514865886\n",
            "\tAverage validation Loss:  0.9335601684374687\n",
            "\tEpoch 41 complete! \tAverage traning Loss:  0.6177958302511146\n",
            "\tAverage validation Loss:  0.9406856218973796\n",
            "\tEpoch 42 complete! \tAverage traning Loss:  0.6120516847435147\n",
            "\tAverage validation Loss:  0.9483987343616974\n",
            "\tEpoch 43 complete! \tAverage traning Loss:  0.6065848919010428\n",
            "\tAverage validation Loss:  0.9553920501317733\n",
            "\tEpoch 44 complete! \tAverage traning Loss:  0.6007977858891395\n",
            "\tAverage validation Loss:  0.9615907180003631\n",
            "\tEpoch 45 complete! \tAverage traning Loss:  0.5954997315048175\n",
            "\tAverage validation Loss:  0.9688922637548202\n",
            "\tEpoch 46 complete! \tAverage traning Loss:  0.5905283556011062\n",
            "\tAverage validation Loss:  0.9779938062032064\n",
            "\tEpoch 47 complete! \tAverage traning Loss:  0.5845170156537324\n",
            "\tAverage validation Loss:  0.9839630298125438\n",
            "\tEpoch 48 complete! \tAverage traning Loss:  0.5797303996710392\n",
            "\tAverage validation Loss:  0.9929950616298577\n",
            "\tEpoch 49 complete! \tAverage traning Loss:  0.5744533872205899\n",
            "\tAverage validation Loss:  0.9991285104017992\n",
            "\tEpoch 50 complete! \tAverage traning Loss:  0.5691514489378438\n",
            "\tAverage validation Loss:  1.0086851144448306\n",
            "\tEpoch 51 complete! \tAverage traning Loss:  0.5643512789585464\n",
            "\tAverage validation Loss:  1.01286102441641\n",
            "\tEpoch 52 complete! \tAverage traning Loss:  0.5589796145978412\n",
            "\tAverage validation Loss:  1.0211026313977363\n",
            "\tEpoch 53 complete! \tAverage traning Loss:  0.5541893913885345\n",
            "\tAverage validation Loss:  1.0304452578226726\n",
            "\tEpoch 54 complete! \tAverage traning Loss:  0.5487736015266694\n",
            "\tAverage validation Loss:  1.0371708869934082\n",
            "\tEpoch 55 complete! \tAverage traning Loss:  0.5444485742733671\n",
            "\tAverage validation Loss:  1.045993474813608\n",
            "\tEpoch 56 complete! \tAverage traning Loss:  0.5391790218672048\n",
            "\tAverage validation Loss:  1.0515007508106722\n",
            "\tEpoch 57 complete! \tAverage traning Loss:  0.5344000103081833\n",
            "\tAverage validation Loss:  1.0588774803357246\n",
            "\tEpoch 58 complete! \tAverage traning Loss:  0.5297262175833615\n",
            "\tAverage validation Loss:  1.06759273822491\n",
            "\tEpoch 59 complete! \tAverage traning Loss:  0.5251768229067492\n",
            "\tAverage validation Loss:  1.0747832273825622\n",
            "\tEpoch 60 complete! \tAverage traning Loss:  0.5203237149708783\n",
            "\tAverage validation Loss:  1.0838492075602213\n",
            "\tEpoch 61 complete! \tAverage traning Loss:  0.5166296350590698\n",
            "\tAverage validation Loss:  1.089727345491067\n",
            "\tEpoch 62 complete! \tAverage traning Loss:  0.5107198088281998\n",
            "\tAverage validation Loss:  1.0986349594898712\n",
            "\tEpoch 63 complete! \tAverage traning Loss:  0.5066259641833292\n",
            "\tAverage validation Loss:  1.1070750994560046\n",
            "\tEpoch 64 complete! \tAverage traning Loss:  0.5023718839236289\n",
            "\tAverage validation Loss:  1.1157202671735715\n",
            "\tEpoch 65 complete! \tAverage traning Loss:  0.49804386415189356\n",
            "\tAverage validation Loss:  1.1225495729690944\n",
            "\tEpoch 66 complete! \tAverage traning Loss:  0.4936207921392074\n",
            "\tAverage validation Loss:  1.1303116920666816\n",
            "\tEpoch 67 complete! \tAverage traning Loss:  0.4889160411271544\n",
            "\tAverage validation Loss:  1.1378566448505107\n",
            "\tEpoch 68 complete! \tAverage traning Loss:  0.4845855040802597\n",
            "\tAverage validation Loss:  1.1484927348601512\n",
            "\tEpoch 69 complete! \tAverage traning Loss:  0.4807025140374484\n",
            "\tAverage validation Loss:  1.1519132614135743\n",
            "\tEpoch 70 complete! \tAverage traning Loss:  0.4763464328638358\n",
            "\tAverage validation Loss:  1.163177235921224\n",
            "\tEpoch 71 complete! \tAverage traning Loss:  0.47251842878655137\n",
            "\tAverage validation Loss:  1.1693319858648838\n",
            "\tEpoch 72 complete! \tAverage traning Loss:  0.4683822072648072\n",
            "\tAverage validation Loss:  1.1789788368420724\n",
            "\tEpoch 73 complete! \tAverage traning Loss:  0.46427157463137486\n",
            "\tAverage validation Loss:  1.1852562977717473\n",
            "\tEpoch 74 complete! \tAverage traning Loss:  0.4602120758099144\n",
            "\tAverage validation Loss:  1.191293056194599\n",
            "\tEpoch 75 complete! \tAverage traning Loss:  0.45532139217621104\n",
            "\tAverage validation Loss:  1.200936212295141\n",
            "\tEpoch 76 complete! \tAverage traning Loss:  0.4519842855777581\n",
            "\tAverage validation Loss:  1.2092773804297814\n",
            "\tEpoch 77 complete! \tAverage traning Loss:  0.4483611012567717\n",
            "\tAverage validation Loss:  1.214708413833227\n",
            "\tEpoch 78 complete! \tAverage traning Loss:  0.44503208129850935\n",
            "\tAverage validation Loss:  1.2247503696343838\n",
            "\tEpoch 79 complete! \tAverage traning Loss:  0.4404207318606151\n",
            "\tAverage validation Loss:  1.2325919493650779\n",
            "\tEpoch 80 complete! \tAverage traning Loss:  0.436709764408866\n",
            "\tAverage validation Loss:  1.238647338671562\n",
            "\tEpoch 81 complete! \tAverage traning Loss:  0.43398095777771933\n",
            "\tAverage validation Loss:  1.2489663001818534\n",
            "\tEpoch 82 complete! \tAverage traning Loss:  0.4291470934089512\n",
            "\tAverage validation Loss:  1.2557295897068121\n",
            "\tEpoch 83 complete! \tAverage traning Loss:  0.42616993796526553\n",
            "\tAverage validation Loss:  1.26455259567652\n",
            "\tEpoch 84 complete! \tAverage traning Loss:  0.42245527954154694\n",
            "\tAverage validation Loss:  1.2692501239287548\n",
            "\tEpoch 85 complete! \tAverage traning Loss:  0.41808057389219494\n",
            "\tAverage validation Loss:  1.283129831460806\n",
            "\tEpoch 86 complete! \tAverage traning Loss:  0.4145856776277335\n",
            "\tAverage validation Loss:  1.2913827186975724\n",
            "\tEpoch 87 complete! \tAverage traning Loss:  0.4113047367350969\n",
            "\tAverage validation Loss:  1.2992720530583308\n",
            "\tEpoch 88 complete! \tAverage traning Loss:  0.4080925832551834\n",
            "\tAverage validation Loss:  1.3042252980745757\n",
            "\tEpoch 89 complete! \tAverage traning Loss:  0.40494031540862696\n",
            "\tAverage validation Loss:  1.313000578758044\n",
            "\tEpoch 90 complete! \tAverage traning Loss:  0.40074999777387443\n",
            "\tAverage validation Loss:  1.321825795295911\n",
            "\tEpoch 91 complete! \tAverage traning Loss:  0.3974185042394569\n",
            "\tAverage validation Loss:  1.3302459447811812\n",
            "\tEpoch 92 complete! \tAverage traning Loss:  0.39426915698728854\n",
            "\tAverage validation Loss:  1.341180287874662\n",
            "\tEpoch 93 complete! \tAverage traning Loss:  0.391968417433311\n",
            "\tAverage validation Loss:  1.347877096518492\n",
            "\tEpoch 94 complete! \tAverage traning Loss:  0.3876696410617457\n",
            "\tAverage validation Loss:  1.3551885213607398\n",
            "\tEpoch 95 complete! \tAverage traning Loss:  0.3844643602132133\n",
            "\tAverage validation Loss:  1.3633203457563352\n",
            "\tEpoch 96 complete! \tAverage traning Loss:  0.3818096227964651\n",
            "\tAverage validation Loss:  1.3725469711499336\n",
            "\tEpoch 97 complete! \tAverage traning Loss:  0.3788068669752158\n",
            "\tAverage validation Loss:  1.3779213783068536\n",
            "\tEpoch 98 complete! \tAverage traning Loss:  0.3751392409662018\n",
            "\tAverage validation Loss:  1.3876281004685622\n",
            "\tEpoch 99 complete! \tAverage traning Loss:  0.37251672226738464\n",
            "\tAverage validation Loss:  1.397193717956543\n",
            "\tEpoch 100 complete! \tAverage traning Loss:  0.36922025209017784\n",
            "\tAverage validation Loss:  1.404013178898738\n",
            "Finish!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iZJ96-Z6Nag_",
        "outputId": "10dd67c3-eba5-48b3-c128-f6275d774cf5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = train_loss_list\n",
        "loss_val = test_loss_list\n",
        "epochs = range(1,101)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwRnRb6CVOOh"
      },
      "source": [
        "torch.save({'model': model.state_dict()}, 'transformer_model.ckpt')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l7Ct2g_WnWE"
      },
      "source": [
        "## Generate image vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdhnhQ6jle5R"
      },
      "source": [
        "Implement in transformer_generate file."
      ]
    }
  ]
}