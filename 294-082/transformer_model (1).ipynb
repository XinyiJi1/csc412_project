{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coFcmMNEYs6s"
      },
      "source": [
        "# CSC294-082 Project (transformer part)\n",
        "#### Author: Xinyi Ji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgxnB9zY2i8"
      },
      "source": [
        "I write the code mainly based on these youtube videos:\n",
        "- https://www.youtube.com/watch?v=U0s0f995w14\n",
        "- http://jalammar.github.io/illustrated-transformer/\n",
        "- https://www.youtube.com/watch?v=4Bdc55j80l8\n",
        "- https://www.youtube.com/watch?app=desktop&v=dichIcUZfOw&ab_channel=Hedu-MathofIntelligence\n",
        "\n",
        "And of course the transformer paper \"Attention is all you need\".\n",
        "\n",
        "The whole model and traning process is the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4K_6g3JZ4D5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import pandas as pd\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37iq9GWf0YLS"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "w_l86AwIYOnN",
        "outputId": "8e0ac453-7e95-4de5-e074-76ecdae99c21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-58995c05-2e2c-47c3-956d-3ed5c9219fe9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1240</th>\n",
              "      <th>1241</th>\n",
              "      <th>1242</th>\n",
              "      <th>1243</th>\n",
              "      <th>1244</th>\n",
              "      <th>1245</th>\n",
              "      <th>1246</th>\n",
              "      <th>1247</th>\n",
              "      <th>1248</th>\n",
              "      <th>1249</th>\n",
              "      <th>1250</th>\n",
              "      <th>1251</th>\n",
              "      <th>1252</th>\n",
              "      <th>1253</th>\n",
              "      <th>1254</th>\n",
              "      <th>1255</th>\n",
              "      <th>1256</th>\n",
              "      <th>1257</th>\n",
              "      <th>1258</th>\n",
              "      <th>1259</th>\n",
              "      <th>1260</th>\n",
              "      <th>1261</th>\n",
              "      <th>1262</th>\n",
              "      <th>1263</th>\n",
              "      <th>1264</th>\n",
              "      <th>1265</th>\n",
              "      <th>1266</th>\n",
              "      <th>1267</th>\n",
              "      <th>1268</th>\n",
              "      <th>1269</th>\n",
              "      <th>1270</th>\n",
              "      <th>1271</th>\n",
              "      <th>1272</th>\n",
              "      <th>1273</th>\n",
              "      <th>1274</th>\n",
              "      <th>1275</th>\n",
              "      <th>1276</th>\n",
              "      <th>1277</th>\n",
              "      <th>1278</th>\n",
              "      <th>1279</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4392</td>\n",
              "      <td>7418</td>\n",
              "      <td>3153</td>\n",
              "      <td>3115</td>\n",
              "      <td>1443</td>\n",
              "      <td>369</td>\n",
              "      <td>802</td>\n",
              "      <td>3078</td>\n",
              "      <td>3029</td>\n",
              "      <td>4466</td>\n",
              "      <td>1327</td>\n",
              "      <td>2226</td>\n",
              "      <td>6839</td>\n",
              "      <td>8106</td>\n",
              "      <td>6384</td>\n",
              "      <td>2773</td>\n",
              "      <td>907</td>\n",
              "      <td>3067</td>\n",
              "      <td>6770</td>\n",
              "      <td>7190</td>\n",
              "      <td>2796</td>\n",
              "      <td>3741</td>\n",
              "      <td>5594</td>\n",
              "      <td>5173</td>\n",
              "      <td>1965</td>\n",
              "      <td>1459</td>\n",
              "      <td>3544</td>\n",
              "      <td>5079</td>\n",
              "      <td>1076</td>\n",
              "      <td>7592</td>\n",
              "      <td>2686</td>\n",
              "      <td>35</td>\n",
              "      <td>684</td>\n",
              "      <td>6332</td>\n",
              "      <td>2248</td>\n",
              "      <td>2410</td>\n",
              "      <td>6438</td>\n",
              "      <td>5232</td>\n",
              "      <td>4340</td>\n",
              "      <td>3732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5348</td>\n",
              "      <td>3623</td>\n",
              "      <td>4011</td>\n",
              "      <td>8135</td>\n",
              "      <td>3104</td>\n",
              "      <td>5908</td>\n",
              "      <td>3940</td>\n",
              "      <td>7692</td>\n",
              "      <td>4905</td>\n",
              "      <td>7557</td>\n",
              "      <td>5127</td>\n",
              "      <td>5896</td>\n",
              "      <td>7110</td>\n",
              "      <td>7207</td>\n",
              "      <td>443</td>\n",
              "      <td>3162</td>\n",
              "      <td>5485</td>\n",
              "      <td>3460</td>\n",
              "      <td>487</td>\n",
              "      <td>4431</td>\n",
              "      <td>39</td>\n",
              "      <td>1414</td>\n",
              "      <td>1121</td>\n",
              "      <td>2716</td>\n",
              "      <td>1830</td>\n",
              "      <td>5256</td>\n",
              "      <td>6160</td>\n",
              "      <td>5740</td>\n",
              "      <td>5740</td>\n",
              "      <td>1378</td>\n",
              "      <td>165</td>\n",
              "      <td>6997</td>\n",
              "      <td>513</td>\n",
              "      <td>5888</td>\n",
              "      <td>715</td>\n",
              "      <td>7221</td>\n",
              "      <td>1477</td>\n",
              "      <td>1718</td>\n",
              "      <td>6809</td>\n",
              "      <td>7624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2412</td>\n",
              "      <td>5539</td>\n",
              "      <td>4720</td>\n",
              "      <td>3537</td>\n",
              "      <td>8118</td>\n",
              "      <td>8013</td>\n",
              "      <td>5092</td>\n",
              "      <td>6220</td>\n",
              "      <td>986</td>\n",
              "      <td>98</td>\n",
              "      <td>6977</td>\n",
              "      <td>6682</td>\n",
              "      <td>6029</td>\n",
              "      <td>4191</td>\n",
              "      <td>4636</td>\n",
              "      <td>7381</td>\n",
              "      <td>4720</td>\n",
              "      <td>5117</td>\n",
              "      <td>7024</td>\n",
              "      <td>7541</td>\n",
              "      <td>7557</td>\n",
              "      <td>2826</td>\n",
              "      <td>4921</td>\n",
              "      <td>672</td>\n",
              "      <td>7247</td>\n",
              "      <td>2132</td>\n",
              "      <td>2389</td>\n",
              "      <td>7232</td>\n",
              "      <td>6865</td>\n",
              "      <td>2706</td>\n",
              "      <td>7549</td>\n",
              "      <td>7539</td>\n",
              "      <td>7562</td>\n",
              "      <td>2655</td>\n",
              "      <td>6077</td>\n",
              "      <td>1851</td>\n",
              "      <td>5687</td>\n",
              "      <td>3320</td>\n",
              "      <td>6326</td>\n",
              "      <td>3814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3433</td>\n",
              "      <td>1107</td>\n",
              "      <td>4013</td>\n",
              "      <td>5119</td>\n",
              "      <td>3028</td>\n",
              "      <td>5468</td>\n",
              "      <td>6997</td>\n",
              "      <td>2793</td>\n",
              "      <td>4569</td>\n",
              "      <td>8160</td>\n",
              "      <td>7131</td>\n",
              "      <td>7123</td>\n",
              "      <td>4735</td>\n",
              "      <td>348</td>\n",
              "      <td>5997</td>\n",
              "      <td>4521</td>\n",
              "      <td>3740</td>\n",
              "      <td>4264</td>\n",
              "      <td>3901</td>\n",
              "      <td>4742</td>\n",
              "      <td>6441</td>\n",
              "      <td>2285</td>\n",
              "      <td>1343</td>\n",
              "      <td>3924</td>\n",
              "      <td>421</td>\n",
              "      <td>7011</td>\n",
              "      <td>423</td>\n",
              "      <td>2764</td>\n",
              "      <td>3477</td>\n",
              "      <td>421</td>\n",
              "      <td>5608</td>\n",
              "      <td>6583</td>\n",
              "      <td>3044</td>\n",
              "      <td>5886</td>\n",
              "      <td>3821</td>\n",
              "      <td>684</td>\n",
              "      <td>117</td>\n",
              "      <td>4036</td>\n",
              "      <td>5671</td>\n",
              "      <td>5066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2654</td>\n",
              "      <td>7520</td>\n",
              "      <td>5405</td>\n",
              "      <td>568</td>\n",
              "      <td>635</td>\n",
              "      <td>6776</td>\n",
              "      <td>3161</td>\n",
              "      <td>7582</td>\n",
              "      <td>410</td>\n",
              "      <td>5107</td>\n",
              "      <td>7229</td>\n",
              "      <td>4605</td>\n",
              "      <td>5914</td>\n",
              "      <td>6660</td>\n",
              "      <td>4809</td>\n",
              "      <td>2411</td>\n",
              "      <td>2684</td>\n",
              "      <td>4236</td>\n",
              "      <td>7716</td>\n",
              "      <td>4150</td>\n",
              "      <td>2817</td>\n",
              "      <td>2496</td>\n",
              "      <td>1789</td>\n",
              "      <td>7018</td>\n",
              "      <td>4355</td>\n",
              "      <td>6600</td>\n",
              "      <td>906</td>\n",
              "      <td>3773</td>\n",
              "      <td>1824</td>\n",
              "      <td>4698</td>\n",
              "      <td>7239</td>\n",
              "      <td>1732</td>\n",
              "      <td>2817</td>\n",
              "      <td>6614</td>\n",
              "      <td>6321</td>\n",
              "      <td>164</td>\n",
              "      <td>5731</td>\n",
              "      <td>4232</td>\n",
              "      <td>5537</td>\n",
              "      <td>7389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3370</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>49</td>\n",
              "      <td>7143</td>\n",
              "      <td>61</td>\n",
              "      <td>7304</td>\n",
              "      <td>5364</td>\n",
              "      <td>2705</td>\n",
              "      <td>2975</td>\n",
              "      <td>1105</td>\n",
              "      <td>1613</td>\n",
              "      <td>459</td>\n",
              "      <td>8111</td>\n",
              "      <td>3401</td>\n",
              "      <td>5390</td>\n",
              "      <td>5652</td>\n",
              "      <td>1035</td>\n",
              "      <td>5322</td>\n",
              "      <td>2854</td>\n",
              "      <td>2765</td>\n",
              "      <td>7188</td>\n",
              "      <td>7415</td>\n",
              "      <td>6735</td>\n",
              "      <td>4255</td>\n",
              "      <td>3541</td>\n",
              "      <td>4007</td>\n",
              "      <td>5232</td>\n",
              "      <td>4194</td>\n",
              "      <td>4796</td>\n",
              "      <td>1824</td>\n",
              "      <td>6082</td>\n",
              "      <td>4011</td>\n",
              "      <td>4353</td>\n",
              "      <td>3528</td>\n",
              "      <td>3576</td>\n",
              "      <td>7910</td>\n",
              "      <td>255</td>\n",
              "      <td>4291</td>\n",
              "      <td>8135</td>\n",
              "      <td>3997</td>\n",
              "      <td>1062</td>\n",
              "      <td>5628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3371</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4017</td>\n",
              "      <td>785</td>\n",
              "      <td>3158</td>\n",
              "      <td>255</td>\n",
              "      <td>6559</td>\n",
              "      <td>4262</td>\n",
              "      <td>3231</td>\n",
              "      <td>7795</td>\n",
              "      <td>773</td>\n",
              "      <td>8002</td>\n",
              "      <td>4584</td>\n",
              "      <td>746</td>\n",
              "      <td>5970</td>\n",
              "      <td>1846</td>\n",
              "      <td>5780</td>\n",
              "      <td>4786</td>\n",
              "      <td>5210</td>\n",
              "      <td>5734</td>\n",
              "      <td>6936</td>\n",
              "      <td>1520</td>\n",
              "      <td>2672</td>\n",
              "      <td>6135</td>\n",
              "      <td>601</td>\n",
              "      <td>3288</td>\n",
              "      <td>8091</td>\n",
              "      <td>2152</td>\n",
              "      <td>3077</td>\n",
              "      <td>1319</td>\n",
              "      <td>686</td>\n",
              "      <td>4670</td>\n",
              "      <td>7059</td>\n",
              "      <td>2894</td>\n",
              "      <td>48</td>\n",
              "      <td>2950</td>\n",
              "      <td>5364</td>\n",
              "      <td>255</td>\n",
              "      <td>3083</td>\n",
              "      <td>4451</td>\n",
              "      <td>3590</td>\n",
              "      <td>6800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3372</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4005</td>\n",
              "      <td>1958</td>\n",
              "      <td>1430</td>\n",
              "      <td>2070</td>\n",
              "      <td>907</td>\n",
              "      <td>2986</td>\n",
              "      <td>5906</td>\n",
              "      <td>6198</td>\n",
              "      <td>7523</td>\n",
              "      <td>7858</td>\n",
              "      <td>6931</td>\n",
              "      <td>1263</td>\n",
              "      <td>5120</td>\n",
              "      <td>5993</td>\n",
              "      <td>3571</td>\n",
              "      <td>304</td>\n",
              "      <td>1941</td>\n",
              "      <td>6580</td>\n",
              "      <td>2115</td>\n",
              "      <td>2639</td>\n",
              "      <td>1822</td>\n",
              "      <td>7989</td>\n",
              "      <td>7491</td>\n",
              "      <td>2650</td>\n",
              "      <td>4575</td>\n",
              "      <td>951</td>\n",
              "      <td>6082</td>\n",
              "      <td>6148</td>\n",
              "      <td>2136</td>\n",
              "      <td>3528</td>\n",
              "      <td>2496</td>\n",
              "      <td>5415</td>\n",
              "      <td>4005</td>\n",
              "      <td>4824</td>\n",
              "      <td>4147</td>\n",
              "      <td>6639</td>\n",
              "      <td>8025</td>\n",
              "      <td>2533</td>\n",
              "      <td>6422</td>\n",
              "      <td>4835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3373</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8023</td>\n",
              "      <td>3851</td>\n",
              "      <td>7190</td>\n",
              "      <td>4383</td>\n",
              "      <td>7484</td>\n",
              "      <td>3959</td>\n",
              "      <td>7230</td>\n",
              "      <td>4698</td>\n",
              "      <td>5275</td>\n",
              "      <td>2553</td>\n",
              "      <td>2260</td>\n",
              "      <td>3490</td>\n",
              "      <td>4237</td>\n",
              "      <td>7798</td>\n",
              "      <td>6307</td>\n",
              "      <td>2513</td>\n",
              "      <td>2777</td>\n",
              "      <td>4233</td>\n",
              "      <td>3684</td>\n",
              "      <td>6950</td>\n",
              "      <td>6141</td>\n",
              "      <td>4478</td>\n",
              "      <td>7545</td>\n",
              "      <td>916</td>\n",
              "      <td>7520</td>\n",
              "      <td>3577</td>\n",
              "      <td>5048</td>\n",
              "      <td>1075</td>\n",
              "      <td>1204</td>\n",
              "      <td>450</td>\n",
              "      <td>5997</td>\n",
              "      <td>2557</td>\n",
              "      <td>7755</td>\n",
              "      <td>649</td>\n",
              "      <td>3635</td>\n",
              "      <td>3433</td>\n",
              "      <td>1944</td>\n",
              "      <td>6135</td>\n",
              "      <td>4950</td>\n",
              "      <td>4312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3374</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1067</td>\n",
              "      <td>154</td>\n",
              "      <td>3494</td>\n",
              "      <td>7451</td>\n",
              "      <td>6195</td>\n",
              "      <td>2398</td>\n",
              "      <td>7064</td>\n",
              "      <td>261</td>\n",
              "      <td>4712</td>\n",
              "      <td>2995</td>\n",
              "      <td>5408</td>\n",
              "      <td>8188</td>\n",
              "      <td>1293</td>\n",
              "      <td>517</td>\n",
              "      <td>4449</td>\n",
              "      <td>6456</td>\n",
              "      <td>3043</td>\n",
              "      <td>263</td>\n",
              "      <td>2861</td>\n",
              "      <td>214</td>\n",
              "      <td>2732</td>\n",
              "      <td>3755</td>\n",
              "      <td>6858</td>\n",
              "      <td>2672</td>\n",
              "      <td>1579</td>\n",
              "      <td>5129</td>\n",
              "      <td>2684</td>\n",
              "      <td>1443</td>\n",
              "      <td>540</td>\n",
              "      <td>1969</td>\n",
              "      <td>4364</td>\n",
              "      <td>7461</td>\n",
              "      <td>6215</td>\n",
              "      <td>6744</td>\n",
              "      <td>2896</td>\n",
              "      <td>4089</td>\n",
              "      <td>3288</td>\n",
              "      <td>5975</td>\n",
              "      <td>932</td>\n",
              "      <td>568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3375 rows Ã— 1280 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58995c05-2e2c-47c3-956d-3ed5c9219fe9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58995c05-2e2c-47c3-956d-3ed5c9219fe9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58995c05-2e2c-47c3-956d-3ed5c9219fe9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      0     1     2     3     4     5     ...  1274  1275  1276  1277  1278  1279\n",
              "0        0     0     0    10     4     0  ...  2248  2410  6438  5232  4340  3732\n",
              "1        0     0     0    12     1     8  ...   715  7221  1477  1718  6809  7624\n",
              "2        0     0     0    10    11     2  ...  6077  1851  5687  3320  6326  3814\n",
              "3        0     0     0    10    15     0  ...  3821   684   117  4036  5671  5066\n",
              "4        0     0     0    10     0     9  ...  6321   164  5731  4232  5537  7389\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "3370     0     0     0     9     8     0  ...   255  4291  8135  3997  1062  5628\n",
              "3371     0     0     0     9     0     8  ...  5364   255  3083  4451  3590  6800\n",
              "3372     0     0     0    10     0    10  ...  4147  6639  8025  2533  6422  4835\n",
              "3373     0     0     0     9    10     0  ...  3635  3433  1944  6135  4950  4312\n",
              "3374     0     0     0     9     0     0  ...  2896  4089  3288  5975   932   568\n",
              "\n",
              "[3375 rows x 1280 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df = pd.read_csv('transformer_data_new.csv')\n",
        "df.columns = range(256+1024)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQL_lHkYe5H"
      },
      "outputs": [],
      "source": [
        "txt_col = range(256)\n",
        "img_col = range(256,1024+256)\n",
        "df_txt = df[txt_col]\n",
        "df_img = df[img_col]\n",
        "txt_array = df_txt.values\n",
        "img_array = df_img.values\n",
        "all_data = df.values\n",
        "all_data = torch.from_numpy(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y2YhDIorU8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80435b2-67ae-425c-ff4e-36d35b717efb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3375"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "all_data.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INMRt3O7VGB8"
      },
      "outputs": [],
      "source": [
        "def split_train_test(data, train_fraq=0.9): \n",
        " n_samples = data.shape[0] \n",
        " data_train = data[:int(n_samples * train_fraq)] \n",
        " data_test = data[int(n_samples * train_fraq):] \n",
        " return data_train, data_test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4idmVl0FrIPY"
      },
      "outputs": [],
      "source": [
        "all_data = split_train_test(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlqe3Jlsrp_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7955b24a-8d20-425d-9d73-6c89b4d863d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3037, 1280])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "all_data[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPDy3-4Dr7yV"
      },
      "outputs": [],
      "source": [
        "train_X = all_data[0][:, :256]\n",
        "train_y = all_data[0][:,256:]\n",
        "test_X =  all_data[1][:, :256]\n",
        "test_y = all_data[1][:,256:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTUEOKcu0eph"
      },
      "outputs": [],
      "source": [
        "def dataloader(dataset, **kwargs): \n",
        "  if 'shuffle' not in kwargs: \n",
        "    kwargs['shuffle'] = True \n",
        "  if 'drop_last' not in kwargs: \n",
        "    kwargs['drop_last'] = True \n",
        "  if 'batch_size' not in kwargs: \n",
        "    kwargs['batch_size'] = 32 \n",
        "  if 'num_workers' not in kwargs: \n",
        "    kwargs['num_workers'] = 8 \n",
        "    kwargs['batch_size'] = min(kwargs['batch_size'], len(dataset)) \n",
        "  return data.DataLoader(dataset, **kwargs) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-vn7Bzc0Xed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c1d5ab-3eb7-43e9-a667-301b934dcba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# Train data is a tensor\n",
        "batch_size = 10\n",
        "num_workers = 8\n",
        "train_dataset = data.TensorDataset(train_X, train_y) \n",
        "test_dataset = data.TensorDataset(test_X, test_y) \n",
        "train_dataloader = dataloader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True) \n",
        "test_dataloader = dataloader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPHnzR0u0LBP"
      },
      "source": [
        "## Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYp5KqMEZ7Nw"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCr_LQk2aDyx"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9fPAnrVaIHp"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length = 256,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opgOtpa8aN2u"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj0zctuvaT1e"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length = 1024,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device),\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, row_mask,col_mask,conv_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        x = self.layers[0](x, enc_out, enc_out, src_mask, row_mask)\n",
        "        x = self.layers[1](x, enc_out, enc_out, src_mask, col_mask)\n",
        "        x = self.layers[2](x, enc_out, enc_out, src_mask, row_mask)\n",
        "        x = self.layers[3](x, enc_out, enc_out, src_mask, row_mask)\n",
        "        x = self.layers[4](x, enc_out, enc_out, src_mask, conv_mask)\n",
        "        res = x[:, -1, :]  # [batch_size, num_class]\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return [out, res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSn6pWBvaYXE"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        device=\"gpu\",\n",
        "        max_length=1024,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            256,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            1024,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_row_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "\n",
        "        # trg_len = 32 * 32 = 1024\n",
        "        assert (\n",
        "            1024 == trg_len\n",
        "        ), \"The size of image vector is 1024\"\n",
        "\n",
        "        y_axis = torch.range(0, 1023).expand(1024,1024)\n",
        "        x_axis = torch.transpose(torch.range(0, 1023).expand(1024,1024), 0, 1)\n",
        "        trg_mask = ((y_axis<x_axis).long() + (y_axis+32>x_axis).long() -1).bool()\n",
        "\n",
        "        trg_mask = trg_mask.expand(N, 1, trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "    \n",
        "    def make_col_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        # trg_len = 32 * 32 = 1024\n",
        "\n",
        "        assert (\n",
        "            1024 == trg_len\n",
        "        ), \"The size of image vector is 1024\"\n",
        "\n",
        "        trg_mask = torch.zeros(1024, 1024)\n",
        "        for i in range(32):\n",
        "          cur_dia = torch.ones((1, 1024-32*i))[0]\n",
        "          cur_mask = torch.diag(cur_dia,-32*i)\n",
        "          trg_mask = trg_mask + cur_mask\n",
        "\n",
        "        trg_mask = trg_mask.expand(N, 1, trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "    \n",
        "    def make_conv_mask(self, trg):\n",
        "      # I am not sure how to deal with this(I searched online without finding the result),hence I will use row mask instead as they have similar performance\n",
        "        N, trg_len = trg.shape\n",
        "        # trg_len = 32 * 32 = 1024\n",
        "        assert (\n",
        "            1024 == trg_len\n",
        "        ), \"The size of image vector is 1024\"\n",
        "\n",
        "        y_axis = torch.range(0, 1023).expand(1024,1024)\n",
        "        x_axis = torch.transpose(torch.range(0, 1023).expand(1024,1024), 0, 1)\n",
        "        trg_mask = ((y_axis<x_axis).long() + (y_axis+32>x_axis).long() -1).bool()\n",
        "\n",
        "        trg_mask = trg_mask.expand(N, 1, trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src_mask = self.make_src_mask(src)\n",
        "        # trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        row_mask = self.make_row_mask(trg)\n",
        "        col_mask = self.make_col_mask(trg)\n",
        "        conv_mask = self.make_conv_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out, res = self.decoder(trg, enc_src, src_mask, row_mask,col_mask,conv_mask)\n",
        "        return [out, res]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xnapAow0RsI"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxUg1sHCXAy3"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "# x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
        "# trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "# src_pad_idx = 0\n",
        "# trg_pad_idx = 0\n",
        "# src_vocab_size = 10\n",
        "# trg_vocab_size = 10\n",
        "# model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "# out = model(x, trg[:, :-1])\n",
        "# print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTFQTLitSkUD"
      },
      "outputs": [],
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# input = torch.tensor([[[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8]],[[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8],[3.2, 1.3,0.2, 0.8]]],dtype=torch.float)\n",
        "# input = torch.transpose(input, 1, 2)\n",
        "# target = torch.tensor([[0,1,2],[2,3,1]], dtype=torch.long)\n",
        "# criterion(input, target).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21k7kcmzqm1j"
      },
      "outputs": [],
      "source": [
        "# input  # batch=2 * voc=4(C) * ques_len=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abi_rYfmu7Bm"
      },
      "outputs": [],
      "source": [
        "# target   # batch=2  * ques_len=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo7qQbYX-BYi"
      },
      "outputs": [],
      "source": [
        "def loss_function(org_img, trg_img):\n",
        "  org_img = org_img[:,1:] # batch(N) * ques_len\n",
        "  trg_img = trg_img[:,:-1, :] # batch(N) * ques_len * voc_size\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  input = trg_img\n",
        "  input = torch.transpose(input, 1, 2)\n",
        "  target = org_img\n",
        "  return criterion(input, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hvwr-VSwxDS",
        "outputId": "66d5c821-66b9-4285-bcba-a433d515cdca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Start training Transformer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch 1 complete! \tAverage traning Loss:  0.8814280708894034\n",
            "\tAverage validation Loss:  0.9020627766847611\n",
            "\tEpoch 2 complete! \tAverage traning Loss:  0.878848991646672\n",
            "\tAverage validation Loss:  0.9020195871591568\n",
            "\tEpoch 3 complete! \tAverage traning Loss:  0.8787272949092435\n",
            "\tAverage validation Loss:  0.9022052198648453\n",
            "\tEpoch 4 complete! \tAverage traning Loss:  0.872508113747401\n",
            "\tAverage validation Loss:  0.8845130532979966\n",
            "\tEpoch 5 complete! \tAverage traning Loss:  0.8523674207017912\n",
            "\tAverage validation Loss:  0.8682185471057892\n",
            "\tEpoch 6 complete! \tAverage traning Loss:  0.8376044677582798\n",
            "\tAverage validation Loss:  0.8570309937000274\n",
            "\tEpoch 7 complete! \tAverage traning Loss:  0.8258345717625902\n",
            "\tAverage validation Loss:  0.8492080748081208\n",
            "\tEpoch 8 complete! \tAverage traning Loss:  0.8170508139970287\n",
            "\tAverage validation Loss:  0.8432326316833496\n",
            "\tEpoch 9 complete! \tAverage traning Loss:  0.8085270935336486\n",
            "\tAverage validation Loss:  0.8382993713021278\n",
            "\tEpoch 10 complete! \tAverage traning Loss:  0.8016421431737231\n",
            "\tAverage validation Loss:  0.8355551421642303\n",
            "\tEpoch 11 complete! \tAverage traning Loss:  0.7952721542080506\n",
            "\tAverage validation Loss:  0.8340093344449997\n",
            "\tEpoch 12 complete! \tAverage traning Loss:  0.7893765209526415\n",
            "\tAverage validation Loss:  0.8340409114956856\n",
            "\tEpoch 13 complete! \tAverage traning Loss:  0.7834978790472675\n",
            "\tAverage validation Loss:  0.8321928948163986\n",
            "\tEpoch 14 complete! \tAverage traning Loss:  0.7777502421511719\n",
            "\tAverage validation Loss:  0.8312670424580574\n",
            "\tEpoch 15 complete! \tAverage traning Loss:  0.7720919429071692\n",
            "\tAverage validation Loss:  0.8316301241517067\n",
            "Finish!!\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 49\n",
        "trg_vocab_size = 8192\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Start training Transformer...\")\n",
        "model.train()\n",
        "\n",
        "global_step = 0\n",
        "pause_iters = 1\n",
        "for epoch in range(15):\n",
        "    overall_loss = 0\n",
        "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
        "        x = x.to(device) \n",
        "        y = y.to(device) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        trg_img, res = model(x, y) # (N, query_len, trg_vocab_size) \n",
        "        loss = loss_function(y, trg_img)\n",
        "        \n",
        "        overall_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage traning Loss: \", overall_loss / (batch_idx*batch_size))\n",
        "    train_loss_list.append(overall_loss / (batch_idx*batch_size))\n",
        "\n",
        "    global_step += 1\n",
        "    if global_step % pause_iters == 0:\n",
        "      test_loss = 0\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch_idx, (x, y) in enumerate(test_dataloader):\n",
        "          x = x.to(device) \n",
        "          y = y.to(device) \n",
        "          trg_img, res = model(x, y) # (N, query_len, trg_vocab_size) \n",
        "          loss = loss_function(y, trg_img)\n",
        "          test_loss += loss.item()   \n",
        "        print(\"\\tAverage validation Loss: \", test_loss / (batch_idx*batch_size))\n",
        "        test_loss_list.append(test_loss / (batch_idx*batch_size))\n",
        "        model.train()\n",
        "print(\"Finish!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZJ96-Z6Nag_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "638e0130-3abc-45df-fb26-3e772fa4eb33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e00e4590e19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loss_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (15,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = train_loss_list\n",
        "loss_val = test_loss_list\n",
        "epochs = range(1,101)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwRnRb6CVOOh"
      },
      "outputs": [],
      "source": [
        "torch.save({'model': model.state_dict()}, 'transformer_model_new.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/My Drive/transformer_model_new.ckpt', 'w') as f:\n",
        "  f.write({'model': model.state_dict()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "WP6tAjGJx1xH",
        "outputId": "0126ebd6-82fd-4c7e-f9ab-128eff0fcd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5ad515ade340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/transformer_model_new.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not dict"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l7Ct2g_WnWE"
      },
      "source": [
        "## Generate image vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j1dnkGTBt9SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdhnhQ6jle5R"
      },
      "source": [
        "Implement in transformer_generate file."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "transformer_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}